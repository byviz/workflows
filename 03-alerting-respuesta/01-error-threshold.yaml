# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKFLOW: Alerta por Umbral de Errores
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
name: error-threshold-alert
description: |
  Workflow que se activa cuando los errores superan un umbral.
  Analiza el contexto y determina severidad automÃ¡ticamente.
enabled: true
tags: ["alerting", "errores", "threshold"]

consts:
  error_threshold: 100
  critical_threshold: 500
  time_window: "5m"

triggers:
  - type: alert  # O manual para testing

steps:
  # 1. Buscar errores en ventana de tiempo
  - name: contar_errores
    type: elasticsearch.search
    with:
      index: "logs-*"
      query:
        bool:
          must:
            - match:
                log.level: "ERROR"
          filter:
            - range:
                "@timestamp":
                  gte: "now-{{consts.time_window}}"
      size: 0
      aggregations:
        por_servicio:
          terms:
            field: "service.name"
            size: 10
        por_tipo:
          terms:
            field: "error.type"
            size: 10
  
  # 2. Evaluar severidad
  - name: evaluar_severidad
    type: if
    with:
      condition: "{{steps.contar_errores.output.hits.total.value >= consts.critical_threshold}}"
      then:
        - type: console
          with:
            message: |
              ğŸš¨ ALERTA CRÃTICA
              Errores: {{steps.contar_errores.output.hits.total.value}}
              Umbral crÃ­tico: {{consts.critical_threshold}}
              
              TOP SERVICIOS AFECTADOS:
              {% for bucket in steps.contar_errores.output.aggregations.por_servicio.buckets limit:5 %}
              - {{bucket.key}}: {{bucket.doc_count}}
              {% endfor %}
              
              TIPOS DE ERROR:
              {% for bucket in steps.contar_errores.output.aggregations.por_tipo.buckets limit:5 %}
              - {{bucket.key}}: {{bucket.doc_count}}
              {% endfor %}
      else:
        - type: console
          with:
            message: |
              âš ï¸  ALERTA MEDIA
              Errores: {{steps.contar_errores.output.hits.total.value}}
              Umbral: {{consts.error_threshold}}
  
  # 3. Buscar contexto adicional
  - name: obtener_contexto
    type: elasticsearch.search
    with:
      index: "logs-*"
      query:
        bool:
          must:
            - match:
                log.level: "ERROR"
          filter:
            - range:
                "@timestamp":
                  gte: "now-{{consts.time_window}}"
      size: 10
      sort:
        - "@timestamp:desc"
  
  # 4. Generar reporte
  - name: reporte_incidente
    type: console
    with:
      message: |
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ğŸ“‹ REPORTE DE INCIDENTE
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        ğŸ• Timestamp: {{now | date: "%Y-%m-%d %H:%M:%S"}}
        â±ï¸  Ventana: Ãšltimos {{consts.time_window}}
        
        ğŸ“Š RESUMEN:
        Total errores: {{steps.contar_errores.output.hits.total.value}}
        Umbral configurado: {{consts.error_threshold}}
        Severidad: {% if steps.contar_errores.output.hits.total.value >= consts.critical_threshold %}CRÃTICA{% else %}MEDIA{% endif %}
        
        ğŸ¢ SERVICIOS MÃS AFECTADOS:
        {% for bucket in steps.contar_errores.output.aggregations.por_servicio.buckets limit:5 %}
        {{loop.index}}. {{bucket.key}}: {{bucket.doc_count}} errores
        {% endfor %}
        
        ğŸ” PRIMEROS ERRORES:
        {% for hit in steps.obtener_contexto.output.hits.hits limit:5 %}
        
        [{{hit._source.@timestamp}}] {{hit._source.service.name | default: "N/A"}}
        {{hit._source.message | truncate: 150}}
        {% endfor %}
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        ğŸ’¡ PRÃ“XIMOS PASOS:
        1. Revisar logs detallados en Kibana
        2. Verificar si servicios estÃ¡n operativos
        3. Contactar equipo responsable si persiste
        4. Documentar en ticket de seguimiento

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CÃ“MO CONFIGURAR:
# 1. Crear alerta en Kibana con condiciÃ³n: count() > 100 en 5min
# 2. AÃ±adir acciÃ³n "Run Workflow" â†’ Seleccionar este workflow
# 3. Ajustar umbrales segÃºn tu entorno
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

